{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyric Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eileenblum/Lyric-Generator/blob/main/Lyric_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiPDV1M_ejns"
      },
      "source": [
        "#References: \n",
        "# guru99 website is a major reference for this code\n",
        "# also  https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "# and https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import nltk\n",
        "import gensim\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import word2vec\n",
        "\n",
        "### New:\n",
        "import tensorflow as tf\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras import layers\n",
        "import time \n",
        "\n",
        "from keras import backend\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from IPython import display\n",
        "###\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import abc\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from gensim.models import Word2Vec\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "##########################  1  #################################################\n",
        "############# Loading the data #################################################\n",
        "################################################################################\n",
        "#establish working directory:\n",
        "#os.chdir(r'C:\\Users\\hprob\\Desktop\\ErdosProjectMay2020\\Sample_project\\Music-Project-master\\Music-Project-master\\Lyrics\\metal_lyrics')\n",
        "#os.getcwd()\n",
        "#entries = os.scandir(r'metal_lyrics')\n",
        "#entries\n",
        "\n",
        "###############  Gather the Metal Files  ################################\n",
        "#Make sure to change this path to where YOU are storing lyrics in your computer    \n",
        "Metal_name_and_text = {}\n",
        "# Walking a directory tree and printing the names of the directories and files\n",
        "for dirpath, dirnames, files in os.walk(r'C:\\Users\\hprob\\Desktop\\ErdosProjectMay2020\\Sample_project\\Music-Project-master\\Music-Project-master\\Lyrics\\metal_lyrics'):\n",
        "    print(f'Found directory: {dirpath}')\n",
        "    for file_name in files:\n",
        "        os.listdir()\n",
        "        print(\"song:\", file_name)\n",
        "        with open(dirpath + r'\\\\' + file_name, \"r\", errors='ignore') as target_file:\n",
        "            Metal_name_and_text[file_name] = target_file.read()\n",
        "    Metal_data = (pd.DataFrame.from_dict(Metal_name_and_text, orient='index')\n",
        "            .reset_index().rename(index=str, columns={'index': 'song_name', 0: 'lyrics'}))\n",
        "\n",
        "Type = np.zeros(shape=(len(Metal_name_and_text),1 ))\n",
        "Metal_data['type'] = Type\n",
        "\n",
        "\n",
        "###############  Gather the Reggae Files  ###############################\n",
        "Reggae_name_and_text = {}\n",
        "DFReggae_name_and_text = pd.DataFrame([['delete','delete','delete','delete','delete']],\n",
        "                                      columns= ['ARTIST_NAME', 'ARTIST_URL', 'SONG_NAME', 'SONG_URL', 'LYRICS'])\n",
        "\n",
        "# Walking a directory tree and printing the names of the directories and files\n",
        "for dirpath, dirnames, files in os.walk(r'C:\\Users\\hprob\\Desktop\\ErdosProjectMay2020\\Sample_project\\Music-Project-master\\Music-Project-master\\Lyrics\\azlyrics-scraper'):\n",
        "    for file_name in files:\n",
        "        Reggae_name_and_text[file_name] = pd.read_csv(r'C:\\Users\\hprob\\Desktop\\ErdosProjectMay2020\\Sample_project\\Music-Project-master\\Music-Project-master\\Lyrics\\azlyrics-scraper' + r'\\\\' + file_name, error_bad_lines=False) \n",
        "        temp = pd.read_csv(r'C:\\Users\\hprob\\Desktop\\ErdosProjectMay2020\\Sample_project\\Music-Project-master\\Music-Project-master\\Lyrics\\azlyrics-scraper' + r'\\\\' + file_name, error_bad_lines=False)\n",
        "        DFReggae_name_and_text = DFReggae_name_and_text.append(temp)  \n",
        "\n",
        "del DFReggae_name_and_text['ARTIST_NAME']\n",
        "del DFReggae_name_and_text['ARTIST_URL']\n",
        "del DFReggae_name_and_text['SONG_URL']\n",
        "DFReggae_name_and_text = DFReggae_name_and_text.reset_index()\n",
        "DFReggae_name_and_text = DFReggae_name_and_text.drop( index = 0)\n",
        "del DFReggae_name_and_text['index']\n",
        "\n",
        "len(DFReggae_name_and_text)\n",
        "Type = np.ones(shape=(len(DFReggae_name_and_text), 1 ))\n",
        "\n",
        "Reggae_data = DFReggae_name_and_text.copy()\n",
        "\n",
        "Reggae_data['type'] = Type####################################\n",
        "Reggae_data.columns = ['song_name', 'lyrics', 'type']\n",
        "del Type\n",
        "\n",
        "\n",
        "##########################  2  #################################################\n",
        "############# Preprocessing the Data ###########################################\n",
        "################################################################################\n",
        "\n",
        "##############  Unionizing data ####\n",
        "#input pds Metal_data and Reggae_data\n",
        "subMetal_data = Metal_data.sample(len(DFReggae_name_and_text))\n",
        "\n",
        "Lyrics = subMetal_data.copy()\n",
        "Lyrics = Lyrics.append(Reggae_data)  #<------------this is the data we will work with\n",
        "Lyrics = Lyrics.dropna()\n",
        "##########################################\n",
        "\n",
        "############  Spliting into train and test ############\n",
        "Lyrics_train, Lyrics_test, type_train, type_test = train_test_split(Lyrics[['song_name','lyrics']],\n",
        "                                                                    Lyrics[['type']], \n",
        "                                                                    stratify=Lyrics[['type']], test_size=0.1)\n",
        "#######################################################\n",
        "\n",
        "################# Cleaning ##############################\n",
        "stop = stopwords.words('english') #importing ENglish stop words\n",
        "#cleaning and lemmatizing Lyrics_train:\n",
        "Pro_Lyrics_train = pd.DataFrame(Lyrics_train).copy()\n",
        "Pro_Lyrics_train['lyrics'] = Pro_Lyrics_train['lyrics'].apply(lambda x: ' '.join(x for x in x.split() if x not in string.punctuation))#All the rows of the text in the data frame is checked for string punctuations, and these are filtered\n",
        "Pro_Lyrics_train['lyrics'] = Pro_Lyrics_train['lyrics'].str.replace('[^\\w\\s]','')#REmoves dots using regular expressions\n",
        "Pro_Lyrics_train['lyrics'] = Pro_Lyrics_train['lyrics'].apply(lambda x:' '.join(x.lower() for x in x.split())) #Converting text to lower case\n",
        "Pro_Lyrics_train['lyrics'] = Pro_Lyrics_train['lyrics'].apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))#Digits are removed from the text\n",
        "Pro_Lyrics_train['lyrics'] = Pro_Lyrics_train['lyrics'].apply(lambda x: ' '.join(x for x in x.split() if not x in stop))#Stop words are removed at this stage\n",
        "Pro_Lyrics_train['lyrics'] = Pro_Lyrics_train['lyrics'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))#Words are filtered now, and different form of the same word is removed using lemmatization\n",
        "Pro_Lyrics_list = []\n",
        "for i in Pro_Lyrics_train['lyrics']:\n",
        "     li = list(i.split(\" \"))\n",
        "     Pro_Lyrics_list.append(li)\t\n",
        "     \n",
        "Pro_Lyrics_train['words'] = Pro_Lyrics_list #Adding the list of words/lemmas used for every song     \n",
        "\n",
        "\n",
        "\n",
        "#cleaning and lemmatizing Lyrics_test:\n",
        "Pro_Lyrics_test = pd.DataFrame(Lyrics_test).copy()\n",
        "Pro_Lyrics_test['lyrics'] = Pro_Lyrics_test['lyrics'].apply(lambda x: ' '.join(x for x in x.split() if x not in string.punctuation))#All the rows of the text in the data frame is checked for string punctuations, and these are filtered\n",
        "Pro_Lyrics_test['lyrics'] = Pro_Lyrics_test['lyrics'].str.replace('[^\\w\\s]','')#REmoves dots using regular expressions\n",
        "Pro_Lyrics_test['lyrics'] = Pro_Lyrics_test['lyrics'].apply(lambda x:' '.join(x.lower() for x in x.split())) #Converting text to lower case\n",
        "Pro_Lyrics_test['lyrics'] = Pro_Lyrics_test['lyrics'].apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))#Digits are removed from the text\n",
        "Pro_Lyrics_test['lyrics'] = Pro_Lyrics_test['lyrics'].apply(lambda x: ' '.join(x for x in x.split() if not x in stop))#Stop words are removed at this stage\n",
        "Pro_Lyrics_test['lyrics'] = Pro_Lyrics_test['lyrics'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))#Words are filtered now, and different form of the same word is removed using lemmatization\n",
        "Pro_Lyrics_test_list = []\n",
        "for i in Pro_Lyrics_test['lyrics']:\n",
        "     li = list(i.split(\" \"))\n",
        "     Pro_Lyrics_test_list.append(li)\t\n",
        "Pro_Lyrics_test['words'] = Pro_Lyrics_test_list #Adding the list of words/lemmas used for every song\n",
        "################################################################################\n",
        "#######  END OF PREPROCESSING  #################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "######################  3  #####################################################\n",
        "############# Training Phase ###################################################\n",
        "################################################################################\n",
        "\n",
        "##########################  Training a couple models  ###################\n",
        "#Training a model with the vocabulary from Pro_Lyrics_list\n",
        "RM_model = Word2Vec(Pro_Lyrics_list, min_count=2, size=150, workers=15, window=15)\n",
        "print()\n",
        "#Saving the model\n",
        "RM_model.save(\"word2vec.RM_model\")\n",
        "RM_model.save(\"RM_model.bin\")\n",
        "print()\n",
        "\n",
        "#Training a model with the imported vocabulary from abc.sents()\n",
        "abc_model = gensim.models.Word2Vec(abc.sents(), min_count=2, size=150, workers=15, window=15)\n",
        "print()\n",
        "#Saving the model\n",
        "abc_model.save(\"word2vec.abc_model\")\n",
        "abc_model.save(\"abc_model.bin\")\n",
        "print()\n",
        "\n",
        "\n",
        "####################### Storing vectors gen by models #################\n",
        "# Store the vectors for train data in following file\n",
        "### Finish <----------------------------------------------------------------------------------------Incomplete\n",
        "#word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
        "#RM_vectors_filename = r'C:\\Users\\hprob\\Desktop\\ErdosProjectMay2020\\Sample_project\\RM_vectors.csv'\n",
        "#with open(RM_vectors_filename, 'w+') as word2vec_file:\n",
        "#    for index, row in Lyrics_train.iterrows():\n",
        "#        model_vector = (np.mean([RM_model[token] for token in row['lyrics']], axis=0)).tolist()\n",
        "#        if index == 0:\n",
        "#            header = \",\".join(str(ele) for ele in range(1000))\n",
        "#            word2vec_file.write(header)\n",
        "#            word2vec_file.write(\"\\n\")\n",
        "#        # Check if the line exists else it is vector of zeros\n",
        "#        if type(model_vector) is list:  \n",
        "#            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "#        else:\n",
        "#            line1 = \",\".join([str(0) for i in range(1000)])\n",
        "#        word2vec_file.write(line1)\n",
        "#        word2vec_file.write('\\n')\n",
        "### Finish <----------------------------------------------------------------------------------------Incomplete\n",
        "################################################################################\n",
        "#######  END OF Training phase  #################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################### 4 ##############################################\n",
        "################## Data Analysis phase #########################################\n",
        "################################################################################\n",
        "\n",
        "##### Similarity testing and playing around ################\n",
        "#word: flesh\n",
        "abc_voc = list(abc_model.wv.vocab)\n",
        "print(\"Most similar to flesh according to abc_model: \")\n",
        "abc_data = abc_model.wv.most_similar('flesh')\n",
        "print(abc_data)\n",
        "\n",
        "RM_voc = list(RM_model.wv.vocab)\n",
        "print(\"Most similar to flesh according to RM_model: \")\n",
        "RM_data = RM_model.wv.most_similar('flesh')\n",
        "print(RM_data)\n",
        "\n",
        "### word: worm\n",
        "abc_voc = list(abc_model.wv.vocab)\n",
        "print(\"Most similar to worm according to abc_model: \")\n",
        "abc_data = abc_model.wv.most_similar('worm')\n",
        "print(abc_data)\n",
        "\n",
        "RM_voc = list(RM_model.wv.vocab)\n",
        "print(\"Most similar to worm according to RM_model: \")\n",
        "RM_data = RM_model.wv.most_similar('worm')\n",
        "print(RM_data)\n",
        "\n",
        "### word: plastic\n",
        "abc_voc = list(abc_model.wv.vocab)\n",
        "print(\"Most similar to plastic according to abc_model: \")\n",
        "abc_data = abc_model.wv.most_similar('plastic')\n",
        "print(abc_data)\n",
        "\n",
        "RM_voc = list(RM_model.wv.vocab)\n",
        "print(\"Most similar to plastic according to RM_model: \")\n",
        "RM_data = RM_model.wv.most_similar('plastic')\n",
        "print(RM_data)\n",
        "\n",
        "### word: hell\n",
        "abc_voc = list(abc_model.wv.vocab)\n",
        "print(\"Most similar to hella ccording to abc_model: \")\n",
        "abc_data = abc_model.wv.most_similar('hell')\n",
        "print(abc_data)\n",
        "\n",
        "RM_voc = list(RM_model.wv.vocab)\n",
        "print(\"Most similar to hell according to RM_model: \")\n",
        "RM_data = RM_model.wv.most_similar('hell')\n",
        "print(RM_data)\n",
        "\n",
        "############ Similarity tests  ##############################\n",
        "similar_words = abc_model.wv.most_similar('god')\t\n",
        "print(similar_words)\n",
        "print()\n",
        "\n",
        "similar_words = RM_model.wv.most_similar('god')\t\n",
        "print(similar_words)\n",
        "print()\n",
        "#############\n",
        "dissimlar_words = abc_model.wv.doesnt_match('god good hell'.split())\n",
        "print(dissimlar_words)\n",
        "print()\n",
        "dissimlar_words = RM_model.wv.doesnt_match('god good hell'.split())\n",
        "print(dissimlar_words)\n",
        "print()\n",
        "\n",
        "#################\n",
        "similarity_two_words = abc_model.wv.similarity('human','rat')\n",
        "print(\"Please provide the similarity between these two words:\")\n",
        "print(similarity_two_words)\n",
        "print()\n",
        "\n",
        "similarity_two_words = RM_model.wv.similarity('human', 'rat')\n",
        "print(\"Please provide the similarity between these two words:\")\n",
        "print(similarity_two_words)\n",
        "print()\n",
        "\n",
        "##########\n",
        "similar = abc_model.wv.similar_by_word('kind')\n",
        "print(similar)\n",
        "print()\n",
        "similar = RM_model.wv.similar_by_word('kind')\n",
        "print(similar)\n",
        "print()\n",
        "\n",
        "\n",
        "###### Loading a model ################################\n",
        "#model = Word2Vec.load('model.bin')\n",
        "#vocab = list(model.wv.vocab)\n",
        "#print()\n",
        "#######################################################\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "#########################  PCA #############################\n",
        "\n",
        "######## passing vectors into df ######################\n",
        "RM_model.wv.vocab\n",
        "RM_model.wv.vectors.shape\n",
        "RM_model.corpus_total_words\n",
        "\n",
        "DF_RM_vectors = pd.DataFrame(data = RM_model.wv.vectors[0:,0:],\n",
        "                index=[i for i in range(RM_model.wv.vectors.shape[0])],\n",
        "                columns=['f'+str(i) for i in range(RM_model.wv.vectors.shape[1])])\n",
        "DF_RM_vectors.tail()\n",
        " \n",
        "\n",
        "abc_model.wv.vocab\n",
        "abc_model.wv.vectors.shape\n",
        "abc_model.corpus_total_words\n",
        "\n",
        "DF_abc_vectors = pd.DataFrame(data = abc_model.wv.vectors[0:,0:],\n",
        "                index=[i for i in range(abc_model.wv.vectors.shape[0])],\n",
        "                columns=['f'+str(i) for i in range(abc_model.wv.vectors.shape[1])])\n",
        "DF_abc_vectors.tail()\n",
        "############################################################\n",
        "\n",
        "########## Re-scaling data ######################################\n",
        "### RM\n",
        "scaler = StandardScaler()\n",
        "# Fit on training set only.\n",
        "scaler.fit(RM_model.wv.vectors)\n",
        "# Apply transform to both the training set and the test set.\n",
        "scaled_RM_vectors = scaler.transform(RM_model.wv.vectors)\n",
        "scaled_RM_vectors  = scaler.transform(RM_model.wv.vectors)\n",
        "\n",
        "RM_pca = PCA(n_components=2)\n",
        "RM_pca.fit(RM_model.wv.vectors)\n",
        "RM_pca.n_components_\n",
        "\n",
        "RM_pca.explained_variance_\n",
        "scaled_RM_vectors = RM_pca.transform(RM_model.wv.vectors)\n",
        "\n",
        "### abc\n",
        "abc_scaler = StandardScaler()\n",
        "# Fit on training set only.\n",
        "abc_scaler.fit(abc_model.wv.vectors)\n",
        "# Apply transform to both the training set and the test set.\n",
        "scaled_abc_vectors = abc_scaler.transform(abc_model.wv.vectors)\n",
        "scaled_RM_vectors  = abc_scaler.transform(abc_model.wv.vectors)\n",
        "\n",
        "abc_pca = PCA(n_components=2)\n",
        "abc_pca.fit(abc_model.wv.vectors)\n",
        "\n",
        "abc_pca.explained_variance_\n",
        "scaled_abc_vectors = abc_pca.transform(abc_model.wv.vectors)\n",
        "\n",
        "#### plotting RM vocabulary  ###############\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.scatter(scaled_RM_vectors[:, 0], scaled_RM_vectors[:, 1])\n",
        "plt.xlabel(\"$PCA_1$\", fontsize=14)\n",
        "plt.ylabel(\"$PCA_2$\", fontsize=14)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "################## clustering ############\n",
        "### specifying some colors\n",
        "number = 100 # of colors/clusters\n",
        "cmap = plt.get_cmap('gnuplot')\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, number)]\n",
        "\n",
        "kmeans = KMeans(number)\n",
        "kmeans.fit(scaled_RM_vectors)\n",
        "RM_clusters = kmeans.predict(scaled_RM_vectors)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i, color in enumerate(colors, start=1):\n",
        "    plt.scatter(scaled_RM_vectors[RM_clusters==i,0], \n",
        "      scaled_RM_vectors[RM_clusters==i,1], label=\"$k$ Cluster \" + str(i),color = color)\n",
        "plt.legend(fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##### plotting abc vocabulary  ###############\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.scatter(scaled_abc_vectors[:, 0], scaled_abc_vectors[:, 1])\n",
        "plt.xlabel(\"$PCA_1$\", fontsize=14)\n",
        "plt.ylabel(\"$PCA_2$\", fontsize=14)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "################## clustering ############\n",
        "### specifying some colors\n",
        "number = 50 # of colors/clusters\n",
        "cmap = plt.get_cmap('gnuplot')\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, number)]\n",
        "\n",
        "kmeans = KMeans(number)\n",
        "kmeans.fit(scaled_abc_vectors)\n",
        "abc_clusters = kmeans.predict(scaled_abc_vectors)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i, color in enumerate(colors, start=1):\n",
        "    plt.scatter(scaled_abc_vectors[abc_clusters==i,0], \n",
        "      scaled_abc_vectors[abc_clusters==i,1], label=\"$k$ Cluster \" + str(i),color = color)\n",
        "plt.legend(fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "##################################################################\n",
        "###### Representing songs as vectors ############################\n",
        "#dropping words NOT in vocabulary\n",
        "\n",
        "count = 0\n",
        "for index, row in Pro_Lyrics_train.iterrows():\n",
        "    count = 0\n",
        "    for w in row['words']:\n",
        "        if w in RM_model.wv.vocab:\n",
        "            print(row['words'][count], \"row: \", count)            \n",
        "            count = count+1           \n",
        "\n",
        "Xx = np.zeros(0)\n",
        "if type([1,2,3,]) is list:\n",
        "        print(\"yes\")\n",
        "\n",
        "###present = np.empty(len(Pro_Lyrics_train), dtype=list)\n",
        "        \n",
        "####### checks for all words that were (not) vectorized ###\n",
        "## stores 0 or 1 in \"in vocabulary\" column\n",
        "turtle = []\n",
        "present = [turtle for i in range(len(Pro_Lyrics_train))]\n",
        "Pro_Lyrics_train['in_vocabulary'] = present\n",
        "Pro_Lyrics_train.tail(5)\n",
        "\n",
        "\n",
        "Zv = [np.zeros(150) for i in range(len(Pro_Lyrics_train))]\n",
        "Pro_Lyrics_train['s_vector'] = Zv\n",
        "\n",
        "Pro_Lyrics_train.sample(3)\n",
        "\n",
        "### Looking for words in RM_model.vocab else, to then delete words\n",
        "count = 0\n",
        "for index, row in Pro_Lyrics_train.iterrows():\n",
        "    count +=1\n",
        "    for w in row['words']:\n",
        "        if w in RM_model.wv.vocab:\n",
        "            #print(RM_model.wv.word_vec(w))\n",
        "            #print('smt')\n",
        "            #print(row['in_vocabulary'])\n",
        "            row['in_vocabulary'] = row['in_vocabulary'] + [1] #means w is part of RM_vocab\n",
        "        else:\n",
        "            row['in_vocabulary'] = row['in_vocabulary'] + [0] #means w is not part of RM_vocab\n",
        "    if(count % 10000 == 0):\n",
        "        print(count)\n",
        "Pro_Lyrics_train.tail(3)\n",
        "\n",
        "print(len(Pro_Lyrics_train['in_vocabulary']))\n",
        "##################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############### representing a song by the average of its words  ##################################\n",
        "# Computing the average of vectorized words in \"in vocabulary\"\n",
        "past_iter = np.zeros(150)\n",
        "count = 0\n",
        "num = 0\n",
        "dummy = 0\n",
        "Pro_Lyrics_train_song_mean = []\n",
        "for index, row in Pro_Lyrics_train.iterrows():\n",
        "    num = 0\n",
        "    temp = np.zeros(150)\n",
        "    #for i in row['in_vocabulary']:\n",
        "    #    if(i == 1):\n",
        "    #        #print(row['words'][i])#=w\n",
        "    #        temp = temp + RM_model.wv.word_vec(row['words'][i])\n",
        "    #        num += 1     \n",
        "    dummy = 0    \n",
        "    for w in row['words']:\n",
        "        if(row['in_vocabulary'][dummy] == 1):\n",
        "            #print(\"w: \", RM_model.wv.word_vec(w))\n",
        "            #print(row['in_vocabulary'][dummy])\n",
        "            #print(w)#row['words'][i])#=w\n",
        "            #print(\"dummy: \", dummy)\n",
        "            temp = temp + RM_model.wv.word_vec(w)\n",
        "            num += 1\n",
        "            #print(\"temp: \",temp)\n",
        "            #print(\"num: \", num)\n",
        "        #else:\n",
        "            #print(\"problematic word!!\")\n",
        "            #print(row['in_vocabulary'][dummy])\n",
        "            #print(w)#row['words'][i])#=w\n",
        "            #print(dummy)    \n",
        "        dummy += 1\n",
        "    if(num == 0):    \n",
        "        print(\"show me the money!\")\n",
        "        row['s_vector'] = np.zeros(150)        \n",
        "    else:\n",
        "        num = num*1.0\n",
        "        #print(\"I know kung fu!\")\n",
        "        #print(\"s_vec: \", row['s_vector'])\n",
        "        #if(count % 1000 == 0):\n",
        "        #     print(\"s_vector: \", row['s_vector'])\n",
        "        #     if ((past_iter==row['s_vector']).all()):\n",
        "        #         print(\"No changes\", )\n",
        "        #     past_iter = row['s_vector']\n",
        "        row['s_vector'] = temp/num\n",
        "        #print(\"s_vec: \", row['s_vector'])\n",
        "    count += 1\n",
        "    if(count % 10000 == 0):\n",
        "        print(count)\n",
        "        print(num)\n",
        "         \n",
        "Pro_Lyrics_train.sample(30)\n",
        "#######################################\n",
        "\n",
        "\n",
        "print(Pro_Lyrics_train['words'][21564][0])\n",
        "del Pro_Lyrics_train['words'][21564][0]\n",
        "##################################################################\n",
        "\n",
        "\n",
        "\n",
        "######### DecisionTree Classifier  ###############\n",
        "Pro_Lyrics_train_clf = DecisionTreeClassifier(random_state = 440)\n",
        "# Fit the model\n",
        "Pro_Lyrics_train_clf.fit(list(Pro_Lyrics_train.s_vector), type_train.type)\n",
        "###########\n",
        "\n",
        "# Plot the fitted tree\n",
        "plt.figure(figsize = (20,20))\n",
        "fig = Pro_Lyrics_train_clf.fit(list(Pro_Lyrics_train.s_vector), type_train.type)\n",
        "tree.plot_tree(fig,filled = True)\n",
        "plt.show()\n",
        "##################        \n",
        "\n",
        "###present = np.empty(len(Pro_Lyrics_train), dtype=list)\n",
        "\n",
        "###Saving the DT model:\n",
        "#import pickle\n",
        "\n",
        "#pkl_filename = \"DT_prolyrics_model.pkl\"\n",
        "#with open(pkl_filename, 'wb') as file:\n",
        "    #pickle.dump(model, file)\n",
        "\n",
        "###\n",
        "\n",
        "####### checks for all words that were (not) vectorized ###\n",
        "## stores 0 or 1 in \"in vocabulary\" column\n",
        "turtle = []\n",
        "present = [turtle for i in range(len(Pro_Lyrics_test))]\n",
        "Pro_Lyrics_test['in_vocabulary'] = present\n",
        "Pro_Lyrics_test.tail(5)\n",
        "\n",
        "\n",
        "Zv = [np.zeros(150) for i in range(len(Pro_Lyrics_test))]\n",
        "Pro_Lyrics_test['s_vector'] = Zv\n",
        "\n",
        "Pro_Lyrics_test.sample(3)\n",
        "\n",
        "### Looking for words in RM_model.vocab else, to then delete words\n",
        "count = 0\n",
        "for index, row in Pro_Lyrics_test.iterrows():\n",
        "    count +=1\n",
        "    for w in row['words']:\n",
        "        if w in RM_model.wv.vocab:\n",
        "            #print(RM_model.wv.word_vec(w))\n",
        "            #print('smt')\n",
        "            #print(row['in_vocabulary'])\n",
        "            row['in_vocabulary'] = row['in_vocabulary'] + [1] #means w is part of RM_vocab\n",
        "        else:\n",
        "            row['in_vocabulary'] = row['in_vocabulary'] + [0] #means w is not part of RM_vocab\n",
        "    if(count % 10000 == 0):\n",
        "        print(count)\n",
        "Pro_Lyrics_test.sample(5)\n",
        "\n",
        "print(len(Pro_Lyrics_test['in_vocabulary']))\n",
        "################\n",
        "\n",
        "\n",
        "############### representing a song by the average of its words  ##################################\n",
        "# Computing the average of vectorized words in \"in vocabulary\"\n",
        "past_iter = np.zeros(150)\n",
        "count = 0\n",
        "num = 0\n",
        "dummy = 0\n",
        "Pro_Lyrics_test_song_mean = []\n",
        "for index, row in Pro_Lyrics_test.iterrows():\n",
        "    num = 0\n",
        "    temp = np.zeros(150)\n",
        "    dummy = 0    \n",
        "    for w in row['words']:\n",
        "        if(row['in_vocabulary'][dummy] == 1):\n",
        "            temp = temp + RM_model.wv.word_vec(w)\n",
        "            num += 1\n",
        "        dummy += 1\n",
        "    if(num == 0):    \n",
        "        print(\"show me the money!\")\n",
        "        row['s_vector'] = np.zeros(150)        \n",
        "    else:\n",
        "        num = num*1.0\n",
        "        row['s_vector'] = temp/num\n",
        "    count += 1\n",
        "    if(count % 1000 == 0):\n",
        "        print(count)\n",
        "        print(num)\n",
        "        \n",
        "Pro_Lyrics_test.sample(10)        \n",
        "#######################################################       \n",
        "\n",
        "######################### Making predictions with Tree ########\n",
        "test_predictions_word2vec = Pro_Lyrics_train_clf.predict(list(Pro_Lyrics_test['s_vector']))\n",
        "\n",
        "print(classification_report(type_test['type'], test_predictions_word2vec))\n",
        "###############################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8dM1eVoqjLc"
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Edit this!!!!:\n",
        "maxlen = 184\n",
        "train_songs_padded = tf.keras.preprocessing.sequence.pad_sequences(songlist, maxlen=maxlen, padding='post').astype('float32')\n",
        "\n",
        "train_songs_padded = tf.expand_dims(train_songs_padded, axis = -1)\n",
        "train_songs_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s93ugqeZrVkM"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_songs_padded).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFZGQcdvqb1j"
      },
      "source": [
        "# Creating a custom activation, a rescaling/shift of tanh so the range is (0,1).\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "\n",
        "def positive_tanh(x):\n",
        "  return 0.5*(tf.nn.tanh(x) + 1)\n",
        "\n",
        "get_custom_objects().update({'positive_tanh': Activation(positive_tanh)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZHI8al2qUXY"
      },
      "source": [
        "# Generator model:\n",
        "\n",
        "def make_generator_model(r):\n",
        "\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(layers.Dense(23*4*256, use_bias=False, kernel_initializer=init, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((23, 4, 256)))\n",
        "    assert model.output_shape == (None, 23, 4, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (3, 3), strides=(1, 1), padding='same', use_bias=False, kernel_initializer=init, kernel_regularizer=tf.keras.regularizers.L2(l2=r))) # was 5,5\n",
        "    assert model.output_shape == (None, 23, 4, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=init, kernel_regularizer=tf.keras.regularizers.L2(l2=r)))\n",
        "    assert model.output_shape == (None, 46, 8, 64) #1\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=init, kernel_regularizer=tf.keras.regularizers.L2(l2=r)))\n",
        "    assert model.output_shape == (None, 92, 16, 32) #1\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # activation was here, and was tanh. We changed to positive_tanh.\n",
        "    model.add(layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=init, kernel_regularizer=tf.keras.regularizers.L2(l2=r)))\n",
        "    model.add(Activation(positive_tanh, name='positive_tanh'))\n",
        "    assert model.output_shape == (None, 184, 32, 1)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2N7nF_4s8Ak"
      },
      "source": [
        "# This is the final activator, turning the array of floats into 0s and 1s, so that \n",
        "# it can actually be read/written as a MIDI file. We have a choice of 4 different rectifiers,\n",
        "# each deciding whether m[i,j] = 0 or 1 based on the matrix's average, nth percentile,\n",
        "# an average of a travelling window, or a fixed value.\n",
        "\n",
        "def avg_rectifier(T):\n",
        "  m = np.array(T)\n",
        "\n",
        "  avg = np.mean(m)\n",
        "\n",
        "  new_m = np.zeros(np.shape(m))\n",
        "\n",
        "  for i in range(m.shape[0]):\n",
        "    for j in range(m.shape[1]):\n",
        "      if m[i,j] > avg:\n",
        "        new_m[i,j] = 1\n",
        "      else:\n",
        "        new_m[i,j] = 0\n",
        "\n",
        "  new_m = tf.expand_dims(new_m, axis = -1)\n",
        "  new_m = tf.expand_dims(new_m, axis = 0)\n",
        "\n",
        "  return new_m\n",
        "\n",
        "\n",
        "def percentile_rectifier(T, perc):\n",
        "  m = np.array(T)\n",
        "\n",
        "  # This gets the perc^th percentile:\n",
        "  percentile = np.percentile(m,perc)\n",
        "\n",
        "  new_m = np.zeros(np.shape(m))\n",
        "\n",
        "  for i in range(m.shape[0]):\n",
        "    for j in range(m.shape[1]):\n",
        "      if m[i,j] > percentile:\n",
        "        new_m[i,j] = 1\n",
        "      else:\n",
        "        new_m[i,j] = 0\n",
        "\n",
        "  new_m = tf.expand_dims(new_m, axis = -1)\n",
        "  new_m = tf.expand_dims(new_m, axis = 0)\n",
        "\n",
        "  return new_m\n",
        "\n",
        "\n",
        "def window_rectifier(T, pad):\n",
        "  m = np.array(T)\n",
        "\n",
        "  new_m = np.zeros(np.shape(m))\n",
        "\n",
        "  for i in range(pad,m.shape[0]-pad):\n",
        "    for j in range(pad,m.shape[1]-pad):\n",
        "      # Here we take the average of a (2*pad+1)x(2*pad+1) window around m[i,j]\n",
        "      L = m[i-pad:i+pad+1,j-pad:j+pad+1]\n",
        "      L[pad,pad] = 0\n",
        "      window_avg = np.sum(L)/((2*pad+1)^2 - 1)\n",
        "\n",
        "      if m[i,j] > window_avg:\n",
        "        new_m[i,j] = 1\n",
        "      else:\n",
        "        new_m[i,j] = 0\n",
        "  \n",
        "  # Here we're losing the information on the edges:\n",
        "  new_m[:pad,:pad] = 0\n",
        "  new_m[m.shape[0]-pad:,m.shape[1]-pad:] = 0\n",
        "\n",
        "  new_m = tf.expand_dims(new_m, axis = -1)\n",
        "  new_m = tf.expand_dims(new_m, axis = 0)\n",
        "\n",
        "  return new_m\n",
        "\n",
        "\n",
        "def absolute_rectifier(T, value):\n",
        "  m = np.array(T)\n",
        "\n",
        "  new_m = np.zeros(np.shape(m))\n",
        "  # value is what decides whether m[i,j] = 0 or 1\n",
        "\n",
        "  for i in range(m.shape[0]):\n",
        "    for j in range(m.shape[1]):\n",
        "      if m[i,j] > value:\n",
        "        new_m[i,j] = 1\n",
        "      else:\n",
        "        new_m[i,j] = 0\n",
        "\n",
        "  new_m = tf.expand_dims(new_m, axis = -1)\n",
        "  new_m = tf.expand_dims(new_m, axis = 0)\n",
        "\n",
        "  return new_m\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zXyD25FrqCK"
      },
      "source": [
        "# implementation of wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io8e6aglqUgV"
      },
      "source": [
        "# Losses:\n",
        "# Note -1 = real, 1 = fake.\n",
        "\n",
        "def discriminator_loss_total(real_output, fake_output):\n",
        "    real_loss = wasserstein_loss(-1*tf.ones_like(real_output), real_output)\n",
        "    fake_loss = wasserstein_loss(tf.ones_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def discriminator_loss_real(real_output):\n",
        "    real_loss = wasserstein_loss(-1*tf.ones_like(real_output), real_output)\n",
        "    return real_loss\n",
        "\n",
        "def discriminator_loss_fake(fake_output):\n",
        "    fake_loss = wasserstein_loss(tf.ones_like(fake_output), fake_output)\n",
        "    return fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return wasserstein_loss(-1*tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARo6YskAqUiW"
      },
      "source": [
        "# DCGANs usually use 100,000 - 500,000 iterations (epochs) for the generator...Colab pro?\n",
        "EPOCHS = 25\n",
        "\n",
        "noise_dim = 100\n",
        "# make this a square (1,4,16,... etc.) for the subplotting code:\n",
        "num_examples_to_generate = 1\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUrU_U8ir3dU"
      },
      "source": [
        "def train(dataset, epochs, val, tag, w1, w2, w3):\n",
        "    n_critic = 5\n",
        "    c_r_hist, c_f_hist, g_hist = list(), list(), list()\n",
        "    # Should the noise go here??? And pass the same noise to the d_train_step and g_train_step???\n",
        "    count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            c_r_tmp, c_f_tmp, g_tmp = list(), list(), list()\n",
        "\n",
        "            for _ in range(n_critic):\n",
        "                c_r_loss, c_f_loss = d_train_step(image_batch)\n",
        "                c_r_tmp.append(c1_r_loss)\n",
        "                c_f_tmp.append(c1_f_loss)\n",
        "\n",
        "            c_r_hist.append(mean(c_r_tmp))\n",
        "            c_f_hist.append(mean(c_f_tmp))\n",
        "\n",
        "            g_loss = g_train_step(w1, w2, w3)\n",
        "            g_hist.append(g_loss)\n",
        "\n",
        "            print('>%d, c1r=%.3f, c1f=%.3f, c2r=%.3f, c2f=%.3f, c3r=%.3f, c3f=%.3f, g=%.3f' % (count+1, c1_r_hist[-1], c1_f_hist[-1], c2_r_hist[-1], c2_f_hist[-1], c3_r_hist[-1], c3_f_hist[-1], g_loss))\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # Save a song every 10 epochs:\n",
        "        #display.clear_output(wait=True)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          generate_and_save(generator, epoch + 1, seed, tag, val)\n",
        "\n",
        "        # Save the model every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "    # Plot the (final) loss history\n",
        "    plot_history(c1_r_hist, c1_f_hist, c2_r_hist, c2_f_hist, c3_r_hist, c3_f_hist, g_hist)\n",
        "    plot_crit_history(c1_r_hist, c1_f_hist, c2_r_hist, c2_f_hist, c3_r_hist, c3_f_hist)\n",
        "    plot_crit_fake_history(c1_f_hist, c2_f_hist, c3_f_hist)\n",
        "    plot_crit_fake_avg_history(c1_f_hist, c2_f_hist, c3_f_hist)\n",
        "\n",
        "    # Generate after the final epoch\n",
        "    #display.clear_output(wait=True)\n",
        "    generate_and_save(generator, epochs, seed, tag, val)\n",
        "\n",
        "    return c_r_hist, c_f_hist, g_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCar_Urdr3ku"
      },
      "source": [
        "def generate_and_save(model, epoch, test_input, tag, val):\n",
        "    # Notice 'training' is set to False.\n",
        "    # This is so all layers run in inference mode (batchnorm).\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(int(np.sqrt(num_examples_to_generate)), int(np.sqrt(num_examples_to_generate))))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(int(np.sqrt(num_examples_to_generate)), int(np.sqrt(num_examples_to_generate)), i+1)\n",
        "        if tag == \"absolute\":\n",
        "          plt.imshow(absolute_rectifier(predictions[i,:,:,0], val)[0,:,:,0], cmap='gray')\n",
        "          plt.axis('off')\n",
        "          noteStateMatrixToMidi(absolute_rectifier(predictions[i,:,:,0], val)[0,:,:,0], \"song \" + str(i) + \" of epoch \" + str(epoch))\n",
        "        elif tag == \"percentile\":\n",
        "          plt.imshow(percentile_rectifier(predictions[i,:,:,0], val)[0,:,:,0], cmap='gray')\n",
        "          plt.axis('off')\n",
        "          noteStateMatrixToMidi(percentile_rectifier(predictions[i,:,:,0], val)[0,:,:,0], \"song \" + str(i) + \" of epoch \" + str(epoch))\n",
        "        elif tag == \"window\":\n",
        "          plt.imshow(window_rectifier(predictions[i,:,:,0], val)[0,:,:,0], cmap='gray')\n",
        "          plt.axis('off')\n",
        "          noteStateMatrixToMidi(window_rectifier(predictions[i,:,:,0], val)[0,:,:,0], \"song \" + str(i) + \" of epoch \" + str(epoch))\n",
        "        elif tag == \"average\":\n",
        "          plt.imshow(avg_rectifier(predictions[i,:,:,0])[0,:,:,0], cmap='gray')\n",
        "          plt.axis('off')\n",
        "          noteStateMatrixToMidi(avg_rectifier(predictions[i,:,:,0])[0,:,:,0], \"song \" + str(i) + \" of epoch \" + str(epoch))\n",
        "\n",
        "    plt.savefig('/content/drive/MyDrive/Music GAN Images/Images/image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMO7sVOCsUZm"
      },
      "source": [
        "# the 3 fields for the generator are: \n",
        "# (L2 regularization parameter for the kernel, T/F value of whether to use kernel constraint, MaxNorm value for the kernel\n",
        "# constraint [if included using the T/F value]).\n",
        "\n",
        "# Here we make the generator and apply it to sample noise:\n",
        "generator = make_generator_model(0.0001)\n",
        "noise = tf.random.normal([1, 100])\n",
        "S = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(S[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqtH4vhysUcL"
      },
      "source": [
        "# Now apply the rectifier to make all entries 0 or 1:\n",
        "newS = percentile_rectifier(S[0,:,:,0], 80)\n",
        "plt.imshow(newS[0,:,:,0], cmap='gray')\n",
        "\n",
        "# Save the test song:\n",
        "noteStateMatrixToMidi(np.array(newS[0,:,:,0]), \"test song\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SlZzm_ksUfb"
      },
      "source": [
        "# Just checking what a real song looks like:\n",
        "plt.imshow(songlist[5][:184,:], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNMSssGdsfj7"
      },
      "source": [
        "# the 4 fields for the discriminators are: \n",
        "# (L2 regularization parameter for the kernel, T/F value of whether to use a kernel constraint, MaxNorm value for the kernel\n",
        "# constraint [if included using the T/F value], and dropout rate).\n",
        "\n",
        "discriminator1 = make_discriminator_model1(0.0001, 0.3)   # Dropout was 0.3 earlier\n",
        "discriminator2 = make_discriminator_model2(0.0001, 0.3)\n",
        "discriminator3 = make_discriminator_model3(0.0001, 0.3)\n",
        "decision1 = discriminator1(newS)\n",
        "decision2 = discriminator2(newS)\n",
        "decision3 = discriminator3(newS)\n",
        "print(np.array(decision1)[0][0], np.array(decision2)[0][0], np.array(decision3)[0][0])\n",
        "print()\n",
        "# positive for real, negative for fake.\n",
        "\n",
        "# weighted sum of 3 decisions:\n",
        "weight1 = 1\n",
        "weight2 = 1.5\n",
        "weight3 = 0.7\n",
        "print((weight1*np.array(decision1)[0][0] + weight2*np.array(decision2)[0][0] + weight3*np.array(decision3)[0][0])/(weight1+weight2+weight3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq9X1S76sfm7"
      },
      "source": [
        "# Try a learning rate schedule (exponential decreasing cyclic)\n",
        "\n",
        "#initial_learning_rate = 0.1\n",
        "#lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, \n",
        "                                                          #decay_steps=100000, \n",
        "                                                          #decay_rate=0.96, \n",
        "                                                          #staircase=True)\n",
        "\n",
        "lr_schedule = tfa.optimizers.ExponentialCyclicalLearningRate(initial_learning_rate=1e-3,\n",
        "                                                             maximal_learning_rate=1e-1,\n",
        "                                                             step_size=2000,\n",
        "                                                             scale_mode=\"cycle\",\n",
        "                                                             gamma=0.90,\n",
        "                                                             name=\"MyCyclicScheduler\")\n",
        "\n",
        "generator_optimizer = RMSprop(learning_rate=lr_schedule)\n",
        "discriminator_optimizer1 = RMSprop(learning_rate=lr_schedule)\n",
        "discriminator_optimizer2 = RMSprop(learning_rate=lr_schedule)\n",
        "discriminator_optimizer3 = RMSprop(learning_rate=lr_schedule)\n",
        "\n",
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer1=discriminator_optimizer1,\n",
        "                                 discriminator_optimizer2=discriminator_optimizer2,\n",
        "                                 discriminator_optimizer3=discriminator_optimizer3,\n",
        "                                 generator=generator,\n",
        "                                 discriminator1=discriminator1,\n",
        "                                 discriminator2=discriminator2,\n",
        "                                 discriminator3=discriminator3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3cPh0w9skf7"
      },
      "source": [
        "# Now for actual training:\n",
        "\n",
        "# Last three numbers are weights for discriminator 1, 2, and 3. The generator's loss function is fed the weighted average of\n",
        "# the decision from the 3 discriminators. Ex.: (w1, w2, w3) = (1, 0, 0) means the generator's loss function is only being\n",
        "# fed the 1st discriminator's decision.\n",
        "# The input before the weights is a tag which identifies which rectifier to use to make sure all entries are 0 or 1.\n",
        "# Available rectifiers: absolute, percentile, window, and average.\n",
        "# Before this is a value which, for the window/absolute/percentile rectifiers, specifies the window padding/\n",
        "# fixed value/percentile which those rectifiers will use.\n",
        "\n",
        "# QUESTION: Should training be True for all parties in each of d_train and g_train??? Or only the neural nets that /are/ being trained there???\n",
        "# What about the \"trainable\" attribute?\n",
        "c_r_hist,c_f_hist,g_hist = train(train_dataset, EPOCHS, 90, \"percentile\", 1, 1, 0.8)\n",
        "\n",
        "# Best results I have found have been with percentile rectifier, 75-95th percentile and\n",
        "# absolute rectifier with fixed value of around 0.15."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnhO7WrUskiF"
      },
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "plt.plot(c_r_hist, label=\"critic real loss\")\n",
        "plt.plot(c_f_hist, label=\"critic fake loss\")\n",
        "plt.plot(g_hist, label=\"generator fake loss\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kun9bFssvS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZkUgkJ4ssyJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}